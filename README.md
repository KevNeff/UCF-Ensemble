# Introduction
Combining neural network models to bolster classification accuracy has become a common place in application driven technologies and competitions. Focusing on networks that have varied design and architecture is one way to improve accuracy and generalize the application of an ensembled set of networks. I proposed applying a three-dimensional convolutional neural network (3D CNN) and a long short-term memory (LSTM) recurrent neural network (RNN) with differing features to make a generalized model for human action recognition. 

# Dataset
The dataset used is the popularized benchmark, UCF-101. This dataset contains 101 classes ranging from ‘ApplyingMakeup’ to ‘Rafting’ to ‘YoYo’. Each class contains multiple groups of videos of various people performing an action, and each group contains several videos of that person performing the same action. Each video is an AVI file of a person performing an action from the third person. More about UCF-101 can be learned at https://www.crcv.ucf.edu/data/UCF101.php.

# Ensemble Model

## 3DCNN
I chose a three-dimensional convolutional neural network (3D CNN) since they have been shown to build a strong temporal memory. Where other models may fail to classify an action when a portion of the frames given were undiscernible, 3D CNNs may succeed in classification if a portion of the frames can be classified with a high confidence level. Most networks with various architectures used in action recognition and even video object detection feed a stack of frames into the neural network or an optical flow stack to achieve this effect. This is done in a 3D CNN I chose by feeding a stack of 10 frames at a time to classify an action. The 3D CNN I used is based on this network. https://github.com/dipakkr/3d-cnn-action-recognition/tree/master/3DCNN-v2.0. It has been modified to take a different shape when predicting so that a single prediction at a time may be made for the purposes of ensembling, as well as to take training and test splits from a file to share the same training test split between networks.  Hyperparameters that I found the most success with were to extract the videos with color, enabling of a skipping architecture in the CNN, and a frame depth of 10 frames deep.  

## LSTM
I chose a long short-term memory (LSTM) recurrent neural network (RNN) due to the novel implementation that I found using a convolutional neural network (CNN) trained on an image dataset for action recognition to extract features from video frames. This CNN was VGG16 trained on the object detection benchmark ImageNet. VGG16 object detection is perfomed on the dataset’s frames and saved so that they may be accessed again. Changes that I made to this neural network involved a full reimplementation on prediction, rewriting the testing phase as well as creating a method within the model’s class for returning the full SoftMax values of a prediction for the purposes of ensembling. Only minor changes to the training and test splitting were made, and the splits were written to file so that they might be used within the 3D CNN training, and all model’s testing. Hyperparameters I found useful were to include the output layer of VGG16 with weights loaded from ImageNet, a batch size of 64, 20 epochs, and use a frame depth of 20 input frames.

## Ensemble
The ensemble reads in a previously generated test set and stores it as a list from file. The ensemble sits in the parent folder of both networks and accesses needed functions and data from within the file tree. It loads both model’s architecture and weights before beginning prediction. In the prediction method, a single test sample from the test list is fed into each neural network, the softmax values of each are then averaged to find the prediction of the ensemble. The current code is implemented to modify a single parameter to get a weighted average. Whenever a prediction of the 3D CNN, LSTM or ensemble matches the correct label, it is recorded. This method is called until every video to be tested has been received. The most recent prediction is written then written to file for access by the GUI which makes singular predictions. Finally, the correct label counts are divided by the length of the test set.  The downstream models have been modified to test and train from shared train test splits available by generation from setgeneration.py

# GUI
The Graphical User Interface (GUI) was written with the tkinter framework. I created a grid layout for components to be placed on. I included, a button to open the dataset folder for selection of a video to test on, a label for the selected test file’s name to appear, the label that was predicted below, and finally below is a frame where the first image frame in the selected video is loaded and appears. This implementation is perfect for demoing the model in demonstrations and includes a flashy yet un-obstructive background to catch a viewer or passerby’s eye. The GUI loads the model into memory for quick results.

![Alt Text](https://github.com/KevNeff/UCF-Ensemble/blob/master/GUI%20example.PNG)
